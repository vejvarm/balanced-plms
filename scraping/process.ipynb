{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b3b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vejvar-martin-nj/git/balanced-plms/scraping\n"
     ]
    }
   ],
   "source": [
    "%cd ~/git/balanced-plms/scraping/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45d0d7",
   "metadata": {},
   "source": [
    "## Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cd7cca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/home/vejvar-martin-nj/git/balanced-plms/scraping/batch_api/scrape_stackoverflow.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!python scrape_stackoverflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a23cd3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vejvar-martin-nj/git/balanced-plms/scraping/batch_api\n",
      "../stackexchange_sparql_rdf_examples.jsonl: 3819it [00:00, 27305.10it/s]\n",
      "Wrote 3819 examples to sparql_stackexchange_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "%cd batch_api/\n",
    "!python 00_prepare_stackexchange.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4aa0f098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vejvar-martin-nj/git/balanced-plms/scraping/batch_api/.helpers\n",
      "Filtered 382 out of 3819 rows. Kept 3437 entries (query length >= 50).\n",
      "/home/vejvar-martin-nj/git/balanced-plms/scraping/batch_api\n"
     ]
    }
   ],
   "source": [
    "%cd .helpers/\n",
    "!python remove_short_queries.py\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d79eb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini-2024-07-18 t=0.15: 100%|█████| 3437/3437 [00:00<00:00, 27512.42it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t015.jsonl\n",
      "gpt-4o-mini-2024-07-18 t=0.3: 100%|██████| 3437/3437 [00:00<00:00, 27941.49it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t03.jsonl\n",
      "gpt-4o-mini-2024-07-18 t=0.45: 100%|█████| 3437/3437 [00:00<00:00, 27937.15it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t045.jsonl\n",
      "gpt-4o-mini-2024-07-18 t=0.6: 100%|██████| 3437/3437 [00:00<00:00, 28067.54it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t06.jsonl\n",
      "gpt-4o-mini-2024-07-18 t=0.75: 100%|█████| 3437/3437 [00:00<00:00, 27797.25it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t075.jsonl\n",
      "gpt-4o-mini-2024-07-18 t=0.9: 100%|██████| 3437/3437 [00:00<00:00, 27795.97it/s]\n",
      "Batch input file ready: .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t09.jsonl\n"
     ]
    }
   ],
   "source": [
    "!python 01_prepare_batches_stackexchange.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45105db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t015.jsonl ...\n",
      "Uploaded file ID: file-GVueTX6rsY6prrg8Leac4c\n",
      "Batch ID: batch_684a8cb4586c8190ad1ea236091e992a\n",
      "Skipping .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t03.jsonl, already submitted.\n",
      "Skipping .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t045.jsonl, already submitted.\n",
      "Skipping .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t06.jsonl, already submitted.\n",
      "Skipping .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t075.jsonl, already submitted.\n",
      "Skipping .batches/openai_batch_input_gpt-4.1-mini-2025-04-14_t09.jsonl, already submitted.\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t015.jsonl ...\n",
      "Uploaded file ID: file-8pGkfqneCxKoiqnH9o1Dyi\n",
      "Batch ID: batch_684a8cb5d5e48190ac0af217499d7193\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t03.jsonl ...\n",
      "Uploaded file ID: file-WjEZZ5FfByjZNkExSPe5Su\n",
      "Batch ID: batch_684a8cb78c7c819096a9a7eaf3c31c91\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t045.jsonl ...\n",
      "Uploaded file ID: file-3ewMxcKEc4MXAr3ELE3wZ6\n",
      "Batch ID: batch_684a8cb962dc81908b7f8b0765802d6d\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t06.jsonl ...\n",
      "Uploaded file ID: file-H43ezLXjaHmekWZgdTC7sH\n",
      "Batch ID: batch_684a8cbb37708190b312c6eaa2aef484\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t075.jsonl ...\n",
      "Uploaded file ID: file-9ZTSvWAhcfQFqHW5AKrNqm\n",
      "Batch ID: batch_684a8cbc742881909f8aedfb1946e86b\n",
      "Uploading .batches/openai_batch_input_gpt-4o-mini-2024-07-18_t09.jsonl ...\n",
      "Uploaded file ID: file-95vbVd4Kn3BzusN6ibZVuJ\n",
      "Batch ID: batch_684a8cbdead0819086d7af2620840eaa\n",
      "All new batches submitted! Check progress at https://platform.openai.com/batches\n"
     ]
    }
   ],
   "source": [
    "!python 02_submit_batches.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7342c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython 03_download_batch_results.py &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "!python 03_download_batch_results.py &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3be5b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a646c80508190ba11b1d347269fe6.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a64786d0c819082699463bce6002d.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a647a40648190a91f91338273827d.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cb4586c8190ad1ea236091e992a.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cb5d5e48190ac0af217499d7193.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cb78c7c819096a9a7eaf3c31c91.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cbb37708190b312c6eaa2aef484.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cbc742881909f8aedfb1946e86b.jsonl\n",
      "Processing: .batch_results/stackexchange/openai_batch_output_batch_684a8cbdead0819086d7af2620840eaa.jsonl\n",
      "Wrote merged paraphrases to stackexchange_paraphrases_multi_temp.jsonl. The total context sample count is 34370.\n"
     ]
    }
   ],
   "source": [
    "# TODO: before you run, move all the completed batch results you want to merge into .batch_results/stackexchange/\n",
    "!python 04_merge_batch_results_stackexchange.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
