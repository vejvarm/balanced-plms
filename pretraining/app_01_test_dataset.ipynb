{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from collators import DataCollatorForT5MLM\n",
    "\n",
    "# To resume from checkpoint with PyTorch > 2.6\n",
    "# Allow torch.load to unpickle numpy._core.multiarray._reconstruct\n",
    "from numpy._core.multiarray import _reconstruct\n",
    "from numpy import ndarray\n",
    "torch.serialization.add_safe_globals([_reconstruct, ndarray])\n",
    "\n",
    "# Load configuration from JSON file.\n",
    "with open(\"./configs/00_config.json\", \"r\") as f:\n",
    "    config_args = json.load(f)\n",
    "\n",
    "dataset_cache_path =  config_args.get(\"dataset\")\n",
    "# dataset_cache_path = config_args.get(\"dataset_cache_path\", \"/work/datasets/Skylion007___openwebtext\")\n",
    "max_seq_length = config_args.get(\"max_seq_length\", 512)\n",
    "\n",
    "# 1. Load preprocessed dataset from disk\n",
    "print(f\"Loading dataset from cache: {dataset_cache_path}\")\n",
    "grouped_dataset = load_dataset(dataset_cache_path)\n",
    "\n",
    "# 2. Tokenizer initialization.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config_args[\"model_name_or_path\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token  # T5 often uses EOS as PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grouped_dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(grouped_dataset['test']):\n",
    "    pass\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(grouped_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in grouped_dataset['train']['text'][:1]:\n",
    "    print(sample)\n",
    "    # print(tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collators import DataCollatorForT5MLM\n",
    "\n",
    "data_collator = DataCollatorForT5MLM(\n",
    "    tokenizer=tokenizer,\n",
    "    noise_density=config_args.get(\"mlm_probability\", 0.15),\n",
    "    mean_noise_span_length=config_args.get(\"mean_noise_span_length\", 3.0),\n",
    "    input_length=max_seq_length,\n",
    "    target_length=max_seq_length,  # Adjust if needed.\n",
    ")\n",
    "\n",
    "# 4. Load model configuration and model.\n",
    "# config = AutoConfig.from_pretrained(config_args[\"model_name_or_path\"])\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/home/vejvar-martin-nj/git/picard/results/t5/unbiased-openwebtext/checkpoint-45200\")\n",
    "\n",
    "# 5. Set up TrainingArguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config_args[\"output_dir\"],\n",
    "    do_train=config_args.get(\"do_train\", True),\n",
    "    do_eval=config_args.get(\"do_eval\", True),\n",
    "    num_train_epochs=config_args.get(\"num_train_epochs\", 4),\n",
    "    per_device_train_batch_size=config_args.get(\"per_device_train_batch_size\", 12),\n",
    "    per_device_eval_batch_size=config_args.get(\"per_device_eval_batch_size\", 16),\n",
    "    gradient_accumulation_steps=config_args.get(\"gradient_accumulation_steps\", 4),\n",
    "    learning_rate=config_args.get(\"learning_rate\", 3e-4),\n",
    "    lr_scheduler_type=config_args.get(\"lr_scheduler_type\", \"constant\"),\n",
    "    weight_decay=config_args.get(\"weight_decay\", 0.01),\n",
    "    logging_steps=config_args.get(\"logging_steps\", 40),\n",
    "    eval_steps=config_args.get(\"eval_steps\", 200),\n",
    "    save_steps=config_args.get(\"save_steps\", 400),\n",
    "    overwrite_output_dir=config_args.get(\"overwrite_output_dir\", False),\n",
    "    resume_from_checkpoint=config_args.get(\"resume_from_checkpoint\", False),\n",
    "    ignore_data_skip=config_args.get(\"ignore_data_skip\", False),\n",
    "    save_total_limit=config_args.get(\"save_total_limit\", 2),\n",
    "    seed=config_args.get(\"seed\", 42),\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,  # Recommended if GPU supports mixed precision\n",
    "    logging_dir=\"../logs\",\n",
    "    report_to=[\"wandb\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=grouped_dataset[\"train\"],\n",
    "    eval_dataset=grouped_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Select a small sample from the training dataset\n",
    "sample_dataset = grouped_dataset[\"train\"].select(range(20))\n",
    "\n",
    "# Create a DataLoader with the data collator\n",
    "sample_loader = DataLoader(sample_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "for batch in sample_loader:\n",
    "    # Move batch to the appropriate device\n",
    "    batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            max_length=128,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "    # Decode inputs, predictions, and labels\n",
    "    decoded_inputs = tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
    "    decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=False)\n",
    "\n",
    "    # Display the results\n",
    "    for inp, label, pred in zip(decoded_inputs, decoded_labels, decoded_preds):\n",
    "        print(f\"Input:\\n{inp}\\n\\nTarget:\\n{label}\\n\\nPrediction:\\n{pred}\\n{'-'*40}\")\n",
    "    break  # Remove this break to process all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from cache: /work/datasets/openwebtext-clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b33cf83fdca46b1a2cb831d3819825b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# To resume from checkpoint with PyTorch > 2.6\n",
    "# Allow torch.load to unpickle numpy._core.multiarray._reconstruct\n",
    "from numpy._core.multiarray import _reconstruct\n",
    "from numpy import ndarray\n",
    "torch.serialization.add_safe_globals([_reconstruct, ndarray])\n",
    "\n",
    "# dataset_cache_path = config_args.get(\"dataset_cache_path\", \"/work/datasets/Skylion007___openwebtext\")\n",
    "\n",
    "# Load configuration from JSON file.\n",
    "with open(\"./configs/00_config.json\", \"r\") as f:\n",
    "    config_args = json.load(f)\n",
    "\n",
    "dataset_cache_path = config_args.get(\"dataset_cache_path\", \"/work/datasets/owt-10k-clean\")\n",
    "max_seq_length = config_args.get(\"max_seq_length\", 512)\n",
    "\n",
    "# 1. Load preprocessed dataset from disk\n",
    "print(f\"Loading dataset from cache: {dataset_cache_path}\")\n",
    "grouped_dataset = load_from_disk(dataset_cache_path)\n",
    "\n",
    "# 2. Tokenizer initialization.\n",
    "tokenizer = AutoTokenizer.from_pretrained(config_args[\"model_name_or_path\"])\n",
    "tokenizer.pad_token = tokenizer.eos_token  # T5 often uses EOS as PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8684583\n",
      "2383\n"
     ]
    }
   ],
   "source": [
    "print(len(grouped_dataset['train']))\n",
    "for ex in grouped_dataset['test']:\n",
    "    print(len(tokenizer.decode(ex['input_ids'], skip_special_tokens=False)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from data_utils import filter_sql_queries, QL_PATTERNS\n",
    "\n",
    "# Combined pattern used in the filter function\n",
    "pattern = re.compile(\"|\".join(QL_PATTERNS), re.IGNORECASE)\n",
    "# pattern = re.compile(\n",
    "#     r\"\\bSELECT\\b.*\\bFROM\\b|\\bMATCH\\b.*\\bRETURN\\b|\\bselect\\b.*\\bwhere\\b\",\n",
    "#     re.IGNORECASE,\n",
    "# )\n",
    "\n",
    "def contains_query_language(example_text):\n",
    "    return not bool(pattern.search(example_text))\n",
    "\n",
    "# Check for any remaining occurrences in the filtered dataset:\n",
    "split = 'test'\n",
    "remaining = []\n",
    "for ex in grouped_dataset[split]:\n",
    "    inp = {\"text\": tokenizer.decode(ex['input_ids'])}\n",
    "    if contains_query_language(inp[\"text\"]):\n",
    "        remaining.append(inp[\"text\"])\n",
    "        # print(remaining)\n",
    "    \n",
    "# remaining = [ex for ex in grouped_dataset[\"test\"] if contains_query_language(tokenizer.decode(ex['input_ids']))]\n",
    "print(f\"Remaining examples with query language mentions: {len(remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Remaining examples with query language mentions: {len(remaining)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
